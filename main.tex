\documentclass[11pt]{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{geometry}
\geometry{a4paper,left=3.18cm,right=3.18cm,top=2.54cm,bottom=2.54cm}
\usepackage{hyperref}

\title{The Future of Hardware-Software Codesign\\for Domain-Specific Systems}
\author{Zhenbang You\\ \href{zyou@stanford.edu}{zyou@stanford.edu}}
\date{}

\begin{document}

\maketitle

\section{Overview}

\subsection{Concentration}
This essay is devoted to figuring out the respective responsibilities of each component and the future trends in hardware-software codesign in the coming years (and maybe decades), and it focuses on domain-specific systems.
By ``domain-specific systems", I am referring to \textbf{domain-specific languages (DSLs), domain specific compilers (DSCs), and domain-specific architectures (DSAs) as a whole}.

Most of the discussions on design and implementations will be dedicated to software;  for hardware, the discussion will be restricted to their responsibilities.

Only computation tasks are considered.
IO intensive tasks are beyond the scope.

As a convention in this essay, ``developers" refers to the developers of systems, while ``programmers" and``users" refer to users of systems.

\subsection{Challenges of Domain-Specific Systems}
Despite their rapid development in recent year, concern remain about their future:
\begin{enumerate}
    \item Can the benefits of such a system justify the cost of its development?
    \item Given that the industry is conservative on programming languages (PLs), will programmers be persuaded into adopting a new DSL?
    \item Niche languages typically have poor toolchains.
\end{enumerate}
These problems are raised from the perspective of programmers.
After all, even though domain-specific systems are often based on DSAs, it is the languages that are what programmers directly deal with.

Although the above issues should be discussed case by case in real scenarios, whether we can find some general guidelines to improve them has a decisive impact on the future of domain-specific systems.

\subsubsection*{Brief Answer to the Second Question}
\begin{enumerate}
    \item DSLs have very different ecological niches than PLs.
    \item DSLs can be (and should be) made simple enough, even much simpler than PLs in most cases.
\end{enumerate}
Many functional languages fail at the second point, while Kotlin (a new PL combining the advantages of Scala, C\#, and Java, and rather easy to learn and use) fails at the first point (its ecological niche is too identical to Java).

% \subsection{Why Can Domain-Specific Systems Succeed in the Future}
% Domain-specific systems are super inspiring and exciting, but to justify the effort of this essay, I still need to briefly discuss why I think they will probably be a huge hit in the future.
% Instead of talking about the advantages of these systems as many people do, I choose to address some of the common concern towards these systems.

% \subsubsection*{Concern 1: By Being Domain-Specific, There Will Not Be Enough Users, Making It Hard to Cover the Cost of Developing Such a System.}
% Development of domain-specific systems can swift enough in the future (I will discuss this in this essay) so that the benefits of such systems can justify its development.
% Admittedly, how to achieve this is a key issue in the future success of domain-specific systems.

% \subsubsection*{Concern 2: Learning a New Language Takes Time. How to Persuade the Programmers?}
% On one hand, DSLs can be more user-friendly (the development of mainstream PLs in the recent two decades makes me believe this, and I will certainly discuss this issue below).
% On the other hand, programming paradigms are limited.
% Put it another way, although there can be thousands of DSLs, there can only be dozens of programming paradigms.
% Therefore, programming models of many DSLs can be similar enough, rendering it feasible for programmers to learn new languages frequently.

% \subsubsection*{Concern 3: Niches Languages Typically Have Poor Toolchains, Rendering it Hard to Use.}
% Development of toolchains should be given enough considerations in the design of DSLs.
% And this can also be developed in similar ways as the development of compilers.
% The toolchains based on LLVM can be good examples.

\subsection{Benchmark}
% Systems should be analyzed in a quantitative ways, and there is no exception for the systems we are dealing with.
I argue that, in sharp contrast to existing benchmarks like SPEC where hand-optimized code is used (the performance of the system is exploited by experts), benchmarks for domain-specific systems should be based on code optimized by compilers.
In other words, \textbf{only performance that can be exploited automatically by compilers matters.}
Otherwise, it would be impossible to accommodate thousands of languages in the computer industry.

I am not denying the importance of manual optimizations in domain-specific systems.
I just want to emphasize the importance of automatic optimizations especially for less popular domain-specific systems.

% The argument above has many implications.
% For instance, since more optimization is taken care of by compilers, techniques like auto-scheduling will play a more important role, which in turn requires us to find better abstraction for our programming model (otherwise it would be impossible or too costly to do such things).

\section{Design Goals}
The goals here refer to the goals of the whole systems. Besides three goals emphasized by PLs (\textbf{performance, productivity, safety}), one goal commonly added for DSLs (\textbf{generality}), and three main goals in computer architecture (\textbf{performance, power, cost}), I argue that another two goals should also be given the same emphasis:
\begin{enumerate}
    \item Cost of developing such a system.
    Domain-specific systems are inherently much less popular than general ones, so the non-recurring expenditure is a serious issue.
    \item Flat learning curve.
    That means, the system should be easy enough for beginners.
\end{enumerate}

\section{General Approach}
Granted, ``codesign" means both the hardware and the software should be considered at the same time, and thus the top-down approach and the bottom-up approach are both indispensable.
However, I argue that, \textbf{for the sake of user-friendliness, the top-down one should be given more priority.}
% If we want programmers to think such a system is easy to learn and easy to use, we had better start from their perspective.
% In the past, many system designers adopted the bottom up ones and overlook the other, resulting in systems that looked really ugly and inconvenient for programmers.
% For example, Intel has many hardware features that may be powerful but are so hard to use by programmers, rendering them white elephants.

For the same reason, the discussion below will be organized in a top-down way.

\section{Languages}
To speed up the development of languages and their toolchains, embedding them in a ``mother languages" is preferable to the external approach.
What's more, the ecosystem of the mother language also helps a lot.

\subsection{Ideal Features of the ``Mother Language"}
Several mainstream languages are frequently used as mother languages: Scala, Python, C\#, C++, Haskell, etc.
In my opinion, Scala is probably the best one since:
\begin{enumerate}
    \item Supporting many programming paradigms.
    As a PL, it is too complicated -- programmers often just need a subset of its features.
    However, to support various kinds of DSLs, this can be a significant strength, e.g., its type system is the most complex one I have ever heard of, but this is really helpful for developing languages of different paradigms.
    This does not contradict the goal of a flat learning curve, since programmers only need a (small) subset of Scala to use a DSL.
    \item Flexibility.
    Although Haskell is even more expressive than Scala, it is no match for the latter in terms of flexibility.
    Scala can not only express what you want, but express it \textbf{in the way you want}.
\end{enumerate}

However, Scala also has several serious drawbacks as a mother languages:
\begin{enumerate}
    \item Slow compilation (due to its complexity).
    But for a DSL, typically only a small subset of its syntax is needed.
    My idea is that language features can be made \textbf{modular} -- extensions should be enabled explicitly (as the case for GHC extension), and some compiler checks should be able to be disabled (Scala has done some of the latter).
    \item Too many features make code reading difficult.
    Others programmers of the same DSL may use complicated features.
    When I was developing on Spark, I knew the syntax of Spark but still obfuscated by others' usage of ``implicit" (an obscure and unique feature of Scala); as a result, I had a difficulty reading others' code.
    % Here is a personal experience:
    % I knew the syntax of Spark, but others used "implicit" (an obscure and unique feature of Scala); as a result, I had a difficulty reading the code.
    This can also be addressed by making language features modular.
    Project owners can designate what extensions can be used.
\end{enumerate}
\subsection{Key Issues of Designing DSLs}
\subsubsection*{Achieving High Performance in Declarative Paradigms}
So far, in mainstream PLs, when it comes to performance, declarative ones are no match for imperative ones.
However, declarative styles is more favorable in terms of productivity, correctness, and verification (the learning curve can also be flatter if properly designed).
I argue that \textbf{DSLs should be kept declarative, and the performance should be achieved by finding proper abstractions and the effort of compilers.}

The performance of functional languages have be improved significantly in recent years, but a gap still remains (no more than 3X at the moment).
From the perspective of parallel computing, the two most important factors of performance are parallelism and locality.
With appropriate abstractions, compilers does a good job in exploiting parallelism, but their optimizations for locality is far worse, and one of the key reasons is the complex memory hierarchy whose performance is often unpredictable (I will further discuss this in the ``Hardware" section).
% With the gap between processors and memory growing, hardware architect are making the memory hierarchy more complex, which becomes a really tough problem for compiler optimizations.
% I think, in the future, we will have larger memory, but the hierarchy will be simpler and thus more predictable, rendering it more feasible for compilers to exploits the memory subsystem and predict its performance, and even do a good job in managing them in software 

% \subsubsection*{Being Functional Is Good, But Not Too Functional}
% Functional styles have been proved to be expressive, efficient and productive.
% However, functional languages are always hard for average programmers.

With memory bandwidth (instead of disks, network, etc) becoming the bottleneck, more projects are switching to C++ or Rust.
For instance, Databricks is rewriting part of Spark in C++.
Admittedly, for mainstream PLs, we still have not figured out how to achieve high performance and safety but still keep the PL simple enough.
However, for DSLs to be adopted, I do not think the C++/Rust-like approach is reasonable -- they are really too erroneous (C++) or too hard (Rust) for average programmers!
Fortunately, DSLs are only responsible for specialized scenarios, leading to possible solutions to this dilemma, e.g., some machine learning DSLs like PyTorch and Jax are both performant and easy to use.
In addition, we also have some requirements for the underlying hardware to make compiler optimizations effective enough (so that programmers can be free from manual optimizations).

\subsubsection*{Different DSLs Can Have Similar Programming Models}
There can be thousands of DSLs, but only dozens of programming paradigms, which implies many DSLs can be similar in programming models.
This leads to significant advantages:
\begin{enumerate}
    \item Swift development of DSCs and toolchains.
    Developers of such a system can benefit from lessons of developing similar ones.
    \item Programmers of DSLs enjoy shorter learning time if they have experiences on similar DSLs.
    \item Similarity of DSLs makes them easier to run on the same DSAs.
\end{enumerate}
\section{Compilers}
\subsubsection*{Carrying out the Most Important Optimizations, but Not Too Many}
The Pareto principle matters.
Swift development of DSCs requires the developers to find the few optimizations that make a huge difference (and it can often be found).
This is especially important for the first few versions of DSCs.
Compilation speed also argue towards this.

If DSLs and DSAs can besimilar to one another, it will also help DSC developers to figure out what optimizations to carry out in a shorter period of time.
\subsubsection*{DSC Generators}
Just like parser generators like YACC, DSCs may also have their generators.
However, in contrast to parser generators, a DSC generator is probably only responsible for generating (part of) compilers for DSLs of similar programming models.

This argument shows again how helpful it will be if many DSLs have similar programming models.
\subsubsection*{Better Compiler Infrastructure for Developing Toolchains}
LLVM has become the infrastructure of many PL toolchains.
We need a similar one (maybe ones) for DSCs, since LLVM is not flexible and convenient enough if we have many DSLs.

Development of toolchains should be considered when designing DSCs; otherwise, programmers will be reluctant to use the DSL.

\subsubsection*{Auto-Scheduling Becoming More Important, Requiring Better Abstractions}
Programmers do not spend the same amount of time on a DSL compared to a mainstream PL.
Therefore, compilers should do more, which means designers of DSLs should find better abstractions to assist compiler auto-scheduling.

Alternatively, hand-optimized libraries can help if most of the scenarios fall into just a few categories and thus users just need off-the-shelf library functions.
\section{Hardware}
\subsubsection*{Memory Bandwidth Becoming the Bottleneck More Often}
Many components of hardware systems can become the bottleneck.
Admittedly, any one below is still the bottleneck in some scenarios, but I focus on their trends below.
\begin{enumerate}
    \item Disk.
    With the rapid development of SSDs and the increasing capacity of main memory, this is becoming less of an issue.
    \item Network.
    Again, with the rapid improvement of Ethernet and other customized network technologies in datacenters, this is also less of an issue now (as proved by the emergence of disaggregated storage). In the era of the advent of MapReduce, network was the greatest bottleneck in distributed computing, but the landscape has changed a lot since then.
    \item Memory latency.
    This is one of the core bottleneck of instruction-level parallelism, but domain-specific systems are often designed to exploiting data-level and task-level parallelism, where latency is less important than bandwidth.
    \item Number of processor cores.
    Processor cores are increasing significantly in recent years in both CPUs and GPUs, especially when large cores are replaced by small ones, which imposes higher requirement on memory bandwidth.
\end{enumerate}
Some emergent technologies like HBM and 3D cache may more or less address this issue.
And I am not sure if there will be other brand-new technologies to improve this.

However, if this can not be addressed, this should no longer be a problem, but a phenomenon to be considered.

\subsubsection*{Larger memory, But Not More Complex Memory Subsystems}
More programs will be memory bound, together with the improvement of the fabrication technology, larger memory (in each layer of the memory hierarchy) is highly anticipated.

However, more complex memory subsystems leads to less predictable performance and harder compiler development, which goes against the argument I raised in ``Benchmark" subsection that for domain-specific systems, it is the performance that can be utilized by the compilers that truly matters.
Besides, if the memory subsystem is regular and predictable enough, compiler-managed cache can be a good idea.

\subsubsection*{More Small Cores Rather Than Fewer Larger Cores}
The difference of die size between smaller cores and larger ones is stunning -- in current Intel CPUs (12th and 13th generation of Core), the die size of a large core is equivalent to that of four small cores.

Small cores are simpler in many ways (e.g., simpler pipeline and cache), thus easier to be exploited by compilers.

\subsubsection*{Gap between Specialized and General Hardware to be Filled}
As far as I am concerned, famous hardware is either rather general (e.g., GPU) or rather specialized (e.g., TPU v1 can only perform three kinds of operations -- convolution, matrix multiplication, and activation \cite{hennessy2017computer}).
In this way, we only afford to build DSAs for very few applications.
This gap may be closed in some way in the future.
DSAs like the ones that can do all kinds of tensor algebra (Sparse Abstract Machine \cite{hsu2022sparse}) are consistent with my argument here.

\bibliographystyle{plain}
\bibliography{reference}

\section*{Postscript}
This essay may be too ambitious while too ambiguous.
By writing down some insights and thinking about related issues, I did learn a lot from this process.
Sorry for the obvious drawbacks of this essay.
\end{document}

